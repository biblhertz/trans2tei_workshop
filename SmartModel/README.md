## How to prepare your smart text for conversion
### Introduction
With the improvement of text recognition technologies, increasingly accurate automatic transcriptions are finally available. However, these transcriptions often require a great deal of post-production work, especially for identifying text structure, titles, footnotes, or special formatting of characters such as italics, superscripts, and uppercase letters, when transformed into editable text. These texts, which work fairly well for full-text search, are not sufficient when the purpose of recognition is a digital edition or reprint, particularly in TEI XML format.
The idea behind a smart model for TEI conversion is to possibly mark all tose elements in the text in a way that is trainable by a Neural Text Recognition Engine.
### 
Neural text recognition machines, precisely because they have been developed to recognize manuscripts, are not predisposed to report text formatting, which is added as annotations to the content. However, this does not mean that they are unable to detect differences, since these are graphically distinguishable elements. All it is needed is a method that allows the model to also return this information along with the text. The solution found is to train a so-called smart model, that is, a neural model that does not just transcribe text but adds information. The most common case of a smart model is one that interprets abbreviations and returns expansions depending on the context. This means that, for example, instead of transcribing "Vitruuio", with the consonantal u, it normalizes to "Vitruvio", or resolves "ꝑ" with "per", "ꝙ" with "quod" and the abbreviations of m and n (ō, ā etc.) with the necessary expansion (on/om, an/am) depending on the context. In our case, however, it is not about textual information, but about graphic type and the additional information had to be conveyed in a different form. When encoding a text in digital format, this type of information is provided by "tags", for example in HTML <i>…</i> is italic, <sup>…</sup> is superscript, and so on. Taking inspiration from the experiences of other users, who indicated multiple spaces between words with parentheses, markers have been added to the transcription of the content to signal the beginning and end of each alteration. Since parentheses, as well as other characters already present in the text, would have made their search ambiguous, the use of UNICODE symbols was preferred, specifically pairs of mathematical symbols, unique in the text and therefore easily replaceable in post-production.
For example, the following markers were used:

- ⌠⌡ (U-2320/U+2321) to indicate italic
- ⊂⊃ (U+2282/U+2283) to indicate superscript text in references to notes in the text
- ⊤⊥ (U+22A4/U+22A5) to indicate superscript text in footnote references
- ₍₎ (U+208D/U+208E) to indicate small caps
- ↾⇃ (U+21BE/U+21C3) to indicate bold
The only precaution was to manually mark before and after each word or group of words for each line, since an initial model, in which italic sentences that wrapped to the next line were marked only at the beginning and end of the clause, had shown to have problems in inserting markings at the end or beginning of the line. In hindsight, this problem is an obvious consequence of the fact that models analyze individual lines and not sets of lines and their content.
